------------------------------------------------------
CHAPTER 07 - DATA CLEANING AND PREPARATION
------------------------------------------------------

- Handling Missing Data

    For numeric data, pandas uses the floating point NaN to represent missing data.
      This can be easily detected.

    >>> string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])
    >>> string_data

    0     aardvark
    1    artichoke
    2          NaN
    3      avocado
    dtype: object
    
    >>> string_data.isnull()
    
    0    False
    1    False
    2     True
    3    False
    dtype: bool


    In pandas, we've adopted a convention (used in R) by referring to missing data as
      NA.  This may refer to data that either doesn't exist or wasn't observed.  

    When cleaning up data for analysis, it is important to do analysis on why there is
      missing data in the first place.  Were there data collection problems?  Are there
      potential biases in the data caused by the missing values?


    # The Python value 'None' is also treated as NA in object arrays
    >>> string_data[0] = None
    >>> string_data.isnull()

    0    False
    1    False
    2     True
    3    False
    dtype: bool



- List of NA Handling Methods

    Argument	    Description
    --------------------------------------------------------------------------------
    dropna	        Filter axis labels based on whether values for each label have 
                      missing data, with varying thresholds for how much missing 
                      data to tolerate.

    fillna	        Fill in missing data with some value or using an interpolation 
                      method such as 'ffill' or 'bfill'.

    isnull	        Return boolean values indicating which values are missing/NA.

    notnull	        Negation of isnull.



- Filtering Out Missing Data in Series

    # Using the 'dropna' method on a Series will return only the non-null data 
    #   and index values

    >>> from numpy import nan as NA
    >>> data = pd.Series([1, NA, 3.5, NA, 7])
    >>> data.drop_na()

    0    1.0
    2    3.5
    4    7.0
    dtype: float64

    # This is equivalent
    >>> data[data.notnull()]



- Filtering Out Missing Data in DataFrames

    With DataFrames, things are a bit more complex.  You may want to drop rows or 
      columns that are all NA or those containing any NAs.  By default, 'dropna'
      drops any row containing a missing value.

    >>> data = data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA],
                                    [NA, NA, NA], [NA, 6.5, 3.]])

    >>> cleaned = data.dropna()
    >>> data
         0    1    2
    0  1.0  6.5  3.0
    1  1.0  NaN  NaN
    2  NaN  NaN  NaN
    3  NaN  6.5  3.0

    >>> cleaned
         0    1    2
    0  1.0  6.5  3.0


    # Passing how='all' will only drop rows that are all NA
    >>> data.dropna(how='all')

    # To drop columns that are all NA, use axis=1
    >>> data.dropna(axis=1, how='all')


    # You can also create a threshold where you only keep rows containing a certain 
    #   number of observations.  To drop rows that contain 2 or more NA values:
    >>> df.dropna(thresh=2)



- Filling In Missing Data

    Instead of filtering out missing data, you may want to fill the holes with some
      default value instead.  This is done with the pandas 'fillna' method.

    # Replace all the NA values with a default value
    >>> df.fillna(0)

    # If you pass in a dict, a different value can be used for each column
    >>> df.fillna({1: 0.5, 2: 0})


    # Normally 'fillna' returns a new object.  To modify the existing object in place,
    #   the 'inplace' option is used.
    >>> _ = df.fillna(0, inplace=True)


    # The same interpolation methods available for reindexing can also be used with
    #   'fillna'.  Passing method='ffill' will take the last non-NA value in the 
    #   column and use it to fill in each cell underneath.  Also passing in the 
    #   'limit' option will fill only a specified number of cells.
    >>> df.fillna(method='ffill')
    >>> df.fillna(method='ffill', limit=2)

    # Or maybe you want to just fill in the missing values with the column mean
    >>> data = pd.Series([1., NA, 3.5, NA, 7])
    >>> data.fillna(data.mean())



- List of 'fillna' Arguments

    Argument	Description
    ----------------------------------------------------------------------------
    value	    Scalar value or dict-like object to use to fill missing values

    method	    Interpolation; by default 'ffill' if function called with no other arguments

    axis	    Axis to fill on; default axis=0

    inplace	    Modify the calling object without producing a copy

    limit	    For forward and backward filling, maximum number of consecutive periods to fill



- Removing Duplicates

    Duplicate rows may be found in DataFrames for any number of reasons.

    # Create a DataFrame with a duplicate row
    >>> data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],
                            'k2': [1, 1, 2, 3, 3, 4, 4]})

    >>> data
        k1  k2
    0  one   1
    1  two   1
    2  one   2
    3  two   3
    4  one   3
    5  two   4
    6  two   4

    >>> data.duplicated()
    0    False
    1    False
    2    False
    3    False
    4    False
    5    False
    6     True
    dtype: bool


    # Sometimes, you just want to drop all the duplicates
    >>> data.drop_duplicates()
        k1  k2
    0  one   1
    1  two   1
    2  one   2
    3  two   3
    4  one   3
    5  two   4

    # Or, you can drop duplicates based on a single column's values
    >>> data['v1'] = range(7)
    >>> data.drop_duplicates(['k1'])
        k1  k2  v1
    0  one   1   0
    1  two   1   1


    # By default, 'duplicated' and 'drop_duplicates' will keep the first observed
    #   value combination.  To keep the last one instead, use keep='last'.
    >>> data.drop_duplicates(['k1', 'k2'], keep='last')



- Transforming Data Using a Function or Mapping

    For many datasets, you may wish to perform some transformation based on the 
      values in an array, Series, or column in a DataFrame.

    # Consider the following data collected about various kinds of meat
    >>> data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',
                                      'Pastrami', 'corned beef', 'Bacon',
                                      'pastrami', 'honey ham', 'nova lox'],
                             'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
    >>> data
     
              food  ounces
    0        bacon     4.0
    1  pulled pork     3.0
    2        bacon    12.0
    3     Pastrami     6.0
    4  corned beef     7.5
    5        Bacon     8.0
    6     pastrami     3.0
    7    honey ham     5.0
    8     nova lox     6.0


    # Now, supposed you wanted to add a column indicating the type of animal that
    #   each food came from.  Here is a mapping for that:
    >>> meat_to_animal = {
         'bacon': 'pig',
         'pulled pork': 'pig',
         'pastrami': 'cow',
         'corned beef': 'cow',
         'honey ham': 'pig',
         'nova lox': 'salmon'
        }


    # One problem we have when matching columns is that some of the foods are
    #   capitalized in data['food'].  So, we'll get a lowercase version of that column.
    >>> lowercased = data['food'].str.lower()

    # Now, we can add the new column
    >>> data['animal'] = lowercased.map(meat_to_animal)


    # Alternatively, we could just pass a function that does all the work
    >>> data['animal'] = data['food'].map(lambda x: meat_to_animal[x.lower()])



- Replacing Values

    Filling in missing data with 'fillna' is actually a special case of the more general
      problem of value replacement.  

    # Create a series
    >>> data = pd.Series([1., -999., 2., -999., -1000., 3.])

    # We may realize that -999 is the sentinel value for missing data
    >>> data.replace(-999, np.nan)
    0       1.0
    1       NaN
    2       2.0
    3       NaN
    4   -1000.0
    5       3.0
    dtype: float64


    # Replace multiple values with a single value
    >>> data.replace([-999, -1000], np.nan)

    # Replace multiple values with a different value for each one
    >>> data.replace([-999, -1000], [np.nan, 0])

    # Or you can pass in a dict instead
    >>> data.replace({-999: np.nan, -1000: 0})



- Renaming Axis Indexes

    Like values in a series, axis labels can also be transformed by a function or mapping
      to produce new, differently-labeled objects.  The axes can also be modified in place
      without creating a new data structure.

    >>> data = pd.DataFrame(np.arange(12).reshape((3, 4)),
                            index=['Ohio', 'Colorado', 'New York'],
                            columns=['one', 'two', 'three', 'four'])


    # Like a Series, the axis indexes have a 'map' method
    >>> transform = lambda x: x[:4].upper()
    >>> data.index.map(transform)

    Index(['OHIO', 'COLO', 'NEW '], dtype='object')


    # You can also assign to index, mofifying the DataFrame in place
    >>> data.index = data.index.map(transform)


    # If you want to create a transformed version of a dataset without modifying the original,
    #   a useful method is 'rename'
    >>> data.rename(index=str.title, columns=str.upper)

          ONE  TWO  THREE  FOUR
    Ohio    0    1      2     3
    Colo    4    5      6     7
    New     8    9     10    11


    # 'rename' can also be used in conjunction with a dict-like object providing new values
    #   for a subset of the axis labels
    >>> data.rename(index={'OHIO': 'INDIANA'}, columns={'three': 'peekaboo'})

             one  two  peekaboo  four
    INDIANA    0    1         2     3
    COLO       4    5         6     7
    NEW        8    9        10    11


    # If you want to modify a dataset in place, can just pass 'inplace=True'
    >>> data.rename(index={'OHIO': 'INDIANA'}, inplace=True)



- Discretization and Binning

    Continuous data is often discretized or otherwise separated into bins for analysis.
      Suppose you have data about a group of people in a study, and you want to group
      them into discrete age buckets:

    >>> ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
    >>> bins = [18, 25, 35, 60, 100]

    # Now we put our data points into bins with the 'cut' function
    >>> cats = pd.cut(ages, bins)
    >>> cats

    [(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], 
     (35, 60], (25, 35]]
    Length: 12
    Categories (4, interval[int64]): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]


    # The object that 'pandas.cut' returns is a special 'Categorical' object.
    # Calling 'codes' will return an array of the bins of each data point
    >>> cats.codes
    array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)

    # 'categories' will give you an array of the bins themselves
    >>> cats.categories
    IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]]
                  closed='right',
                  dtype='interval[int64]')

    # 'value_counts' returns the list of bins and their counts
    >>> pd.value_counts(cats)


    # By default, the intervals are open on the left side and closed on the right side [)
    #   To switch this, you can pass 'right=False'
    >>> pd.cut(ages, [18, 26, 36, 61, 100], right=False)


    # You can pass your own bin names to the 'labels' option
    >>> group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']
    >>> pd.cut(ages, bins, labels=group_names)

    [Youth, Youth, Youth, YoungAdult, Youth, ..., YoungAdult, Senior, MiddleAged, Mid
     dleAged, YoungAdult]
    Length: 12
    Categories (4, object): [Youth < YoungAdult < MiddleAged < Senior]


    # If you pass an integer number of bins rather than explicit bin edges, that number
    #   of equal-size bins will be created
    >>> data = np.random.rand(20)
    >>> pd.cut(data, 4, precision=2)

    [(0.34, 0.55], (0.34, 0.55], (0.76, 0.97], (0.76, 0.97], (0.34, 0.55], ..., (0.34
    , 0.55], (0.34, 0.55], (0.55, 0.76], (0.34, 0.55], (0.12, 0.34]]
    Length: 20
    Categories (4, interval[float64]): [(0.12, 0.34] < (0.34, 0.55] < (0.55, 0.76] < 
    (0.76, 0.97]]


    # The 'qcut' function bins the data based on sample quantiles
    >>> data = np.random.randn(1000)
    >>> cats = pd.qcut(data, 4)
    >>> cats

    [(-0.0265, 0.62], (0.62, 3.928], (-0.68, -0.0265], (0.62, 3.928], (-0.0265, 0.62]
    , ..., (-0.68, -0.0265], (-0.68, -0.0265], (-2.95, -0.68], (0.62, 3.928], (-0.68, -0.0265]]
    Length: 1000
    Categories (4, interval[float64]): [(-2.95, -0.68] < (-0.68, -0.0265] < (-0.0265, 0.62] <
                                    (0.62, 3.928]]

    >>> pd.value_counts(cats)

    (0.62, 3.928]       250
    (-0.0265, 0.62]     250
    (-0.68, -0.0265]    250
    (-2.95, -0.68]      250
    dtype: int64


    # You can also pass in your own quantiles
    >>> pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])



- Detecting and Filtering Outliers

    Filtering or transforming outliers is largely a matter of applying array operations.


    # Create a DataFrame with some normally distributed data
    >>> data = pd.DataFrame(np.random.randn(1000, 4))

    # Find values in one of the columns exceeding 3 in absolute values
    >>> col = data[2]
    >>> col[np.abs(col) > 3]

    41    -3.399312
    136   -3.745356
    Name: 2, dtype: float64


    # Select all rows having a value exceeding 3 or -3
    >>> data[(np.abs(data) > 3).any(1)]


    # Cap values at 3 or -3
    >>> data[np.abs(data) > 3] = np.sign(data) * 3



- Permutation and Random Sampling

    Permuting (randomly reordering) a Series or the rows in a DataFrame is easy to do
      using the 'numpy.random.permutation' function.


    # Create a DataFrame
    >>> df = pd.DataFrame(np.arange(20).reshape((5, 4)))

    # Get a random permutation of the column indexes
    >>> sampler = np.random.permutation(5)
    >>> sampler
    array([3, 1, 4, 2, 0])

    # Reorder the columns using the permutation array
    >>> df.take(sampler)

        0   1   2   3
    3  12  13  14  15
    1   4   5   6   7
    4  16  17  18  19
    2   8   9  10  11
    0   0   1   2   3


    # We can also select a random subset without replacement
    >>> df.sample(n=3)

        0   1   2   3
    3  12  13  14  15
    4  16  17  18  19
    2   8   9  10  11


    # Or we can select a random sample with replacement
    >>> choices = pd.Series([5, 7, -1, 6, 4])
    >>> draws = df.sample(n=10, replace=True)
    >>> draws

    4    4
    1    7
    4    4
    2   -1
    0    5
    3    6
    1    7
    4    4
    0    5
    4    4
    dtype: int64



- Computing Indicator/Dummy Variables

    Another type of transformation for statistical modeling or machine learning applications
      is converting a categorical variable into a 'dummy' or 'indicator' matrix.  If a column
      in a DataFrame has k distinct values, you would derive a matrix with k columns containing
      all 1's and 0's.  pandas has a 'get_dummies' function for doing this.

    >>> df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'], 'data1': range(6)})
    >>> df

      key  data1
    0   b      0
    1   b      1
    2   a      2
    3   c      3
    4   a      4
    5   b      5


    # Get dummy matrix
    >>> pd.get_dummies(df['key'])

       a  b  c
    0  0  1  0
    1  0  1  0
    2  1  0  0
    3  0  0  1
    4  1  0  0
    5  0  1  0


    # Add a prefix to columns in indicator matrix, which can then be merged with other data.
    >>> dummies = pd.get_dummies(df['key', prefix='key'])
    >>> df_with_dummy = df[['data1']].join(dummies)
    >>> df_with_dummy

       data1  key_a  key_b  key_c
    0      0      0      1      0
    1      1      0      1      0
    2      2      1      0      0
    3      3      0      0      1
    4      4      1      0      0
    5      5      0      1      0 



- Example - Dummy Variables for Movie Genres

    # Load the MovieLens dataset
    >>> mnames = ['movie_id', 'title', 'genres']
    >>> movies = pd.read_table('datasets/movielens/movies.dat', sep='::', 
                               header=None, names=mnames)
    >>> movies[:10]

       movie_id                               title                        genres
    0         1                    Toy Story (1995)   Animation|Children's|Comedy
    1         2                      Jumanji (1995)  Adventure|Children's|Fantasy
    2         3             Grumpier Old Men (1995)                Comedy|Romance
    3         4            Waiting to Exhale (1995)                  Comedy|Drama
    4         5  Father of the Bride Part II (1995)                        Comedy
    5         6                         Heat (1995)         Action|Crime|Thriller
    6         7                      Sabrina (1995)                Comedy|Romance
    7         8                 Tom and Huck (1995)          Adventure|Children's
    8         9                 Sudden Death (1995)                        Action
    9        10                    GoldenEye (1995)     Action|Adventure|Thriller


    # Getting our indicator matrix here requires some wrangling.
    #   First, we we extract the list of unique genres
    >>> all_genres = []
    >>> for x in movies.genres: all_genres.extend(x.split('|'))
    >>> genres = pd.unique(all_genres)
    >>> genres

    array(['Animation', "Children's", 'Comedy', 'Adventure', 'Fantasy',
           'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror',
           'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir',
           'Western'], dtype=object)


    # One way to construct the indicator DataFrame is to start with all zeros
    >>> zero_matrix = np.zeros((len(movies), len(genres)))
    >>> dummies = pd.DataFrame(zero_matrix, columns=genres)
    >>> dummies


    # Now, iterate through each movie and set entries in each row of 'dummies' to 1
    >>> gen = movies.genres[0]
    >>> gen.split('|')
    ['Animation', "Children's", 'Comedy']

    >>> dummies.columns.get_indexer(gen.split('|'))
    array([0, 1, 2])


    # Then, we can use 'iloc' to set values based on the indexes
    >>> for i, gen in enumerate(movies.genres):
            indices = dummies.columns.get_indexer(gen.split('|'))
            dummies.iloc[i, indices] = 1


    # Then, you combine this with 'movies'
    >>> movies_windic = movies.join(dummies.add_prefix('Genre_'))



- Using String Manipulation


- Using Regular Expressions


- List of Regular Expression Methods


- Vectorized String Functions in pandas


- List of Vectorized String Methods