------------------------------------------------------
CHAPTER 07 - DATA CLEANING AND PREPARATION
------------------------------------------------------

- Handling Missing Data

    For numeric data, pandas uses the floating point NaN to represent missing data.
      This can be easily detected.

    >>> string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])
    >>> string_data

    0     aardvark
    1    artichoke
    2          NaN
    3      avocado
    dtype: object
    
    >>> string_data.isnull()
    
    0    False
    1    False
    2     True
    3    False
    dtype: bool


    In pandas, we've adopted a convention (used in R) by referring to missing data as
      NA.  This may refer to data that either doesn't exist or wasn't observed.  

    When cleaning up data for analysis, it is important to do analysis on why there is
      missing data in the first place.  Were there data collection problems?  Are there
      potential biases in the data caused by the missing values?


    # The Python value 'None' is also treated as NA in object arrays
    >>> string_data[0] = None
    >>> string_data.isnull()

    0    False
    1    False
    2     True
    3    False
    dtype: bool



- List of NA Handling Methods

    Argument	    Description
    --------------------------------------------------------------------------------
    dropna	        Filter axis labels based on whether values for each label have 
                      missing data, with varying thresholds for how much missing 
                      data to tolerate.

    fillna	        Fill in missing data with some value or using an interpolation 
                      method such as 'ffill' or 'bfill'.

    isnull	        Return boolean values indicating which values are missing/NA.

    notnull	        Negation of isnull.



- Filtering Out Missing Data in Series

    # Using the 'dropna' method on a Series will return only the non-null data 
    #   and index values

    >>> from numpy import nan as NA
    >>> data = pd.Series([1, NA, 3.5, NA, 7])
    >>> data.drop_na()

    0    1.0
    2    3.5
    4    7.0
    dtype: float64

    # This is equivalent
    >>> data[data.notnull()]



- Filtering Out Missing Data in DataFrames

    With DataFrames, things are a bit more complex.  You may want to drop rows or 
      columns that are all NA or those containing any NAs.  By default, 'dropna'
      drops any row containing a missing value.

    >>> data = data = pd.DataFrame([[1., 6.5, 3.], [1., NA, NA],
                                    [NA, NA, NA], [NA, 6.5, 3.]])

    >>> cleaned = data.dropna()
    >>> data
         0    1    2
    0  1.0  6.5  3.0
    1  1.0  NaN  NaN
    2  NaN  NaN  NaN
    3  NaN  6.5  3.0

    >>> cleaned
         0    1    2
    0  1.0  6.5  3.0


    # Passing how='all' will only drop rows that are all NA
    >>> data.dropna(how='all')

    # To drop columns that are all NA, use axis=1
    >>> data.dropna(axis=1, how='all')


    # You can also create a threshold where you only keep rows containing a certain 
    #   number of observations.  To drop rows that contain 2 or more NA values:
    >>> df.dropna(thresh=2)



- Filling In Missing Data

    Instead of filtering out missing data, you may want to fill the holes with some
      default value instead.  This is done with the pandas 'fillna' method.

    # Replace all the NA values with a default value
    >>> df.fillna(0)

    # If you pass in a dict, a different value can be used for each column
    >>> df.fillna({1: 0.5, 2: 0})


    # Normally 'fillna' returns a new object.  To modify the existing object in place,
    #   the 'inplace' option is used.
    >>> _ = df.fillna(0, inplace=True)


    # The same interpolation methods available for reindexing can also be used with
    #   'fillna'.  Passing method='ffill' will take the last non-NA value in the 
    #   column and use it to fill in each cell underneath.  Also passing in the 
    #   'limit' option will fill only a specified number of cells.
    >>> df.fillna(method='ffill')
    >>> df.fillna(method='ffill', limit=2)

    # Or maybe you want to just fill in the missing values with the column mean
    >>> data = pd.Series([1., NA, 3.5, NA, 7])
    >>> data.fillna(data.mean())



- List of 'fillna' Arguments

    Argument	Description
    ----------------------------------------------------------------------------
    value	    Scalar value or dict-like object to use to fill missing values

    method	    Interpolation; by default 'ffill' if function called with no other arguments

    axis	    Axis to fill on; default axis=0

    inplace	    Modify the calling object without producing a copy

    limit	    For forward and backward filling, maximum number of consecutive periods to fill



- Removing Duplicates

    Duplicate rows may be found in DataFrames for any number of reasons.

    # Create a DataFrame with a duplicate row
    >>> data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],
                            'k2': [1, 1, 2, 3, 3, 4, 4]})

    >>> data
        k1  k2
    0  one   1
    1  two   1
    2  one   2
    3  two   3
    4  one   3
    5  two   4
    6  two   4

    >>> data.duplicated()
    0    False
    1    False
    2    False
    3    False
    4    False
    5    False
    6     True
    dtype: bool


    # Sometimes, you just want to drop all the duplicates
    >>> data.drop_duplicates()
        k1  k2
    0  one   1
    1  two   1
    2  one   2
    3  two   3
    4  one   3
    5  two   4

    # Or, you can drop duplicates based on a single column's values
    >>> data['v1'] = range(7)
    >>> data.drop_duplicates(['k1'])
        k1  k2  v1
    0  one   1   0
    1  two   1   1


    # By default, 'duplicated' and 'drop_duplicates' will keep the first observed
    #   value combination.  To keep the last one instead, use keep='last'.
    >>> data.drop_duplicates(['k1', 'k2'], keep='last')



- Transforming Data Using a Function or Mapping

    For many datasets, you may wish to perform some transformation based on the 
      values in an array, Series, or column in a DataFrame.

    # Consider the following data collected about various kinds of meat
    >>> data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',
                                      'Pastrami', 'corned beef', 'Bacon',
                                      'pastrami', 'honey ham', 'nova lox'],
                             'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
    >>> data
     
              food  ounces
    0        bacon     4.0
    1  pulled pork     3.0
    2        bacon    12.0
    3     Pastrami     6.0
    4  corned beef     7.5
    5        Bacon     8.0
    6     pastrami     3.0
    7    honey ham     5.0
    8     nova lox     6.0


    # Now, supposed you wanted to add a column indicating the type of animal that
    #   each food came from.  Here is a mapping for that:
    >>> meat_to_animal = {
         'bacon': 'pig',
         'pulled pork': 'pig',
         'pastrami': 'cow',
         'corned beef': 'cow',
         'honey ham': 'pig',
         'nova lox': 'salmon'
        }


    # One problem we have when matching columns is that some of the foods are
    #   capitalized in data['food'].  So, we'll get a lowercase version of that column.
    >>> lowercased = data['food'].str.lower()

    # Now, we can add the new column
    >>> data['animal'] = lowercased.map(meat_to_animal)


    # Alternatively, we could just pass a function that does all the work
    >>> data['animal'] = data['food'].map(lambda x: meat_to_animal[x.lower()])
