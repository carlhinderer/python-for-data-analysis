------------------------------------------------------
CHAPTER 06 - DATA LOADING AND STORAGE
------------------------------------------------------

- Parsing Functions in pandas

    Function	     Description
    --------------------------------------------------------------------------------------
    read_csv	     Load delimited data from a file, URL, or file-like object; use comma 
                       as default delimiter

    read_table	     Load delimited data from a file, URL, or file-like object; use tab ('\t') 
                       as default delimiter

    read_fwf	     Read data in fixed-width column format (i.e., no delimiters)

    read_clipboard	 Version of read_table that reads data from the clipboard; useful for 
                       converting tables from web pages

    read_excel	     Read tabular data from an Excel XLS or XLSX file

    read_hdf	     Read HDF5 files written by pandas

    read_html	     Read all tables found in the given HTML document

    read_json	     Read data from a JSON (JavaScript Object Notation) string representation

    read_msgpack	 Read pandas data encoded using the MessagePack binary format

    read_pickle	     Read an arbitrary object stored in Python pickle format

    read_sas	     Read a SAS dataset stored in one of the SAS systemâ€™s custom storage formats

    read_sql	     Read the results of a SQL query (using SQLAlchemy) as a pandas DataFrame

    read_stata	     Read a dataset from Stata file format

    read_feather	 Read the Feather binary file format



- Reading from CSV

    # Here we have a sample csv
    >>> !cat examples/ex1.csv

    a,b,c,d,message
    1,2,3,4,hello
    5,6,7,8,world
    9,10,11,12,foo


    # Here we read the csv into a DataFrame
    >>> df = pd.read_csv('examples/ex1.csv')
    >>> df

       a   b   c   d message
    0  1   2   3   4   hello
    1  5   6   7   8   world
    2  9  10  11  12     foo


    # Or, we can use 'read_table' and specify the delimiter, which will give
    #   us the same DataFrame
    >>> pd.read_table('examples/ex1.csv', sep=',')


    # Sometimes files don't have a header row
    >>> !cat examples/ex2.csv

    1,2,3,4,hello
    5,6,7,8,world
    9,10,11,12,foo

    # In this case, you can either let pandas assign default column names
    >>> pd.read_csv('examples/ex2.csv', header=None)

       0   1   2   3      4
    0  1   2   3   4  hello
    1  5   6   7   8  world
    2  9  10  11  12    foo

    # Or you can specify the column names yourself
    >>> pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])



- Specifying Index and Columns

    # You can specify that one of the columns should be the index
    >>> names = ['a', 'b', 'c', 'd', 'e']
    >>> pd.read_csv('examples/ex2.csv', names=names, index_col='message')

             a   b   c   d
    message               
    hello    1   2   3   4
    world    5   6   7   8
    foo      9  10  11  12


    # A list of column names can be passed to form a hierarchical index of multiple columns
    >>> !cat examples/csv_mindex.csv

    key1,key2,value1,value2
    one,a,1,2
    one,b,3,4
    one,c,5,6
    one,d,7,8
    two,a,9,10
    two,b,11,12
    two,c,13,14
    two,d,15,16

    >>> parsed = pd.read_csv('examples/csv_mindex.csv', index_col=['key1', 'key2'])
    >>> parsed
    
                   value1  value2
    key1 key2                
    one  a          1       2
         b          3       4
         c          5       6
         d          7       8
    two  a          9      10
         b         11      12
         c         13      14
         d         15      16



- Parsing Input

    # In some cases, a table may not have a fixed delimiter, for instance using a 
    #   variable amount of whitespace to separate fields.  Regex's can be used
    #   in this case.
    >>> result = pd.read_table('examples/ex3.txt', sep='\s+')


    # The 'skiprows' option can be used to skip rows without data
    >>> pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3])



- Handling Missing Values

    Missing data is usually either not present (just an empty string) or marked by
      some kind of sentinel value (like 'NA' or 'NULL').

    >>> !cat examples/ex5.csv

    something,a,b,c,d,message
    one,1,2,3,4,NA
    two,5,6,,8,world
    three,9,10,11,12,foo

    >>> result = pd.read_csv('examples/ex5.csv')
    >>> result

      something  a   b     c   d message
    0       one  1   2   3.0   4     NaN
    1       two  5   6   NaN   8   world
    2     three  9  10  11.0  12     foo

    >>> pd.is_null(result)

           something      a      b      c      d  message
    0      False      False  False  False  False     True
    1      False      False  False   True  False    False
    2      False      False  False  False  False    False


    # The 'na_values' option can take either a list or set of strings to consider 
    #   missing values.  (NaN will be inserted in those cells.)
    >>> result = pd.read_csv('examples/ex5.csv', na_values=['NULL'])

    # Different 'na_values' can be specified for each column also
    >>> sentinels = {'message': ['foo', 'NA'], 'something': ['two']}
    >>> pd.read_csv('examples/ex5.csv', na_values=sentinels)



- List of 'read_csv' and 'read_table' Arguments

    Argument	           Description
    ---------------------------------------------------------------------------------------------
    path	               String indicating filesystem location, URL, or file-like object

    sep or delimiter	   Character sequence or regular expression to use to split fields in each row

    header	               Row number to use as column names; defaults to 0 (first row), but should 
                             be None if there is no header row

    index_col	           Column numbers or names to use as the row index in the result; can be a 
                             single name/number or a list of them for a hierarchical index

    names	               List of column names for result, combine with header=None

    skiprows	           Number of rows at beginning of file to ignore or list of row numbers 
                             (starting from 0) to skip.

    na_values	           Sequence of values to replace with NA.

    comment	               Character(s) to split comments off the end of lines.

    parse_dates	           Attempt to parse data to datetime; False by default. If True, will attempt 
                             to parse all columns.  Otherwise can specify a list of column numbers 
                             or name to parse. If element of list is tuple or list, will combine 
                             multiple columns together and parse to date (e.g., if date/time split 
                             across two columns).

    keep_date_col	       If joining columns to parse date, keep the joined columns; False by default.

    converters	           Dict containing column number of name mapping to functions (e.g., {'foo': f} 
                             would apply the function f to all values in the 'foo' column).

    dayfirst	           When parsing potentially ambiguous dates, treat as international format 
                             (e.g., 7/6/2012 -> June 7, 2012); False by default.

    date_parser	           Function to use to parse dates.

    nrows	               Number of rows to read from beginning of file.

    iterator	           Return a TextParser object for reading file piecemeal.

    chunksize	           For iteration, size of file chunks.

    skip_footer	           Number of lines to ignore at end of file.

    verbose	               Print various parser output information, like the number of missing values 
                             placed in non-numeric columns.

    encoding	           Text encoding for Unicode (e.g., 'utf-8' for UTF-8 encoded text).

    squeeze	               If the parsed data only contains one column, return a Series.

    thousands	           Separator for thousands (e.g., ',' or '.').



- Reading Text Files in Pieces

    When processing very large files, you may want to only read a small part of the file,
      or to iterate through chunks of the file.

    # We can change the pandas display setting when looking at large files
    >>> pd.options.display.max_rows = 10

    # If we want to read only a small number of rows from the file, we can use 'nrows'
    >>> pd.read_csv('examples/ex6.csv', nrows=5)


    # To read a file in pieces, specify a 'chunksize' as a number of rows
    >>> chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)

    >>> tot = pd.Series([])
    >>> for piece in chunker:
            tot = tot.add(piece[key].value_counts(), fill_value=0)
    >>> tot = tot.sort_values(ascending=False)



- Writing Data to Text Format

    # Read a csv file
    >>> data = pd.read_csv('examples/ex5.csv')
    >>> data

    # Write the DataFrame to a csv
    >>> data.to_csv('examples/out.csv')


    # Different outputs and separators can be used
    >>> data.to_csv(sys.stdout, sep='|')

    # The value that can be inserted for missing values is set with 'na_rep'
    >>> data.to_csv(sys.stdout, na_rep='NULL')

    # Both row and column labels are written by default, but both can be disabled
    >>> data.to_csv(sys.stdout, index=False, header=False)

    # Just a subset of columns can be written
    >>> data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c')


    # A Series object can also be written to csv
    >>> dates = pd.date_range('1/1/2000', periods=7)
    >>> ts = pd.Series(np.arange(7), index=dates)
    >>> ts.to_csv('examples/tseries.csv')

    >>> !cat examples/tseries.csv

    2000-01-01,0
    2000-01-02,1
    2000-01-03,2
    2000-01-04,3
    2000-01-05,4
    2000-01-06,5
    2000-01-07,6



- Working With Delimited Formats

    Sometimes, a file will contain malformed lines that trip up the pandas data reader.
      In these cases, it's often easier to iterate through the file by hand using
      Python's built-in 'csv' module.

    # Simple example
    >>> import csv
    >>> f = open('examples/ex7.csv')
    >>> reader = csv.reader(f)

    >>> for line in reader:
            print(line)

    ['a', 'b', 'c']
    ['1', '2', '3']
    ['1', '2', '3']


    # An example where more wrangling is required
    >>> with open('examples/ex7.csv') as f:
            lines = list(csv.reader(f))

    >>> header, values = lines[0], lines[1:]
    >>> data_dict = {h: v for h, v in zip(header, zip(*values))}


    # To define a custom csv parsing format with a different delimeter, string-quoting
    #   convention, and line terminator, we can define a subclass of 'csv.Dialect'.
    >>> class MyDialect(csv.Dialect):
            lineterminator = '\n'
            delimiter = ';'
            quotechar = '"'
            quoting = csv.QUOTE_MINIMAL

    >>> reader = csv.reader(f, dialect=MyDialect)


    # The 'csv.writer' can be used to write files, and can also use a dialect
    >>> with open('mydata.csv', 'w') as f:
            writer = csv.writer(f, dialect=my_dialect)
            writer.writerow(('one', 'two', 'three'))
            writer.writerow(('1', '2', '3'))
            writer.writerow(('4', '5', '6'))
            writer.writerow(('7', '8', '9'))



- List of CSV Dialect Options

    Argument	        Description
    ------------------------------------------------------------------------------------
    delimiter	        One-character string to separate fields; defaults to ','.

    lineterminator	    Line terminator for writing; defaults to '\r\n'. Reader ignores this 
                          and recognizes cross-platform line terminators.

    quotechar	        Quote character for fields with special characters (like a delimiter); 
                          default is '"'.

    quoting	            Quoting convention. Options include csv.QUOTE_ALL (quote all fields),
                          csv.QUOTE_MINIMAL (only fields with special characters like the delimiter),
                          csv.QUOTE_NONNUMERIC, and csv.QUOTE_NONE (no quoting). See Pythonâ€™s 
                          documentation for full details. Defaults to QUOTE_MINIMAL.

    skipinitialspace	Ignore whitespace after each delimiter; default is False.

    doublequote	        How to handle quoting character inside a field; if True, it is doubled 
                          (see online documentation for full detail and behavior).

    escapechar	        String to escape the delimiter if quoting is set to csv.QUOTE_NONE; 
                          disabled by default.



- JSON Data

    # Create a JSON string
    >>> """
    {"name": "Wes",
     "places_lived": ["United States", "Spain", "Germany"],
     "pet": null,
     "siblings": [{"name": "Scott", "age": 30, "pets": ["Zeus", "Zuko"]},
                  {"name": "Katie", "age": 38,
                   "pets": ["Sixes", "Stache", "Cisco"]}]
    }
    """

    # Load the JSON string into Python
    >>> import json
    >>> result = json.loads(obj)
    >>> result
    {'name': 'Wes',
     'pet': None,
     'places_lived': ['United States', 'Spain', 'Germany'],
     'siblings': [{'age': 30, 'name': 'Scott', 'pets': ['Zeus', 'Zuko']},
                  {'age': 38, 'name': 'Katie', 'pets': ['Sixes', 'Stache', 'Cisco']}]}

    # 'json.dumps' converts a Python string back to JSON
    >>> asjson = json.dumps(result)


    # Series and DataFrame objects can automatically read JSON
    >>> !cat examples/example.json

    [{"a": 1, "b": 2, "c": 3},
     {"a": 4, "b": 5, "c": 6},
     {"a": 7, "b": 8, "c": 9}]

    >>> data = pd.read_json('examples/example.json')
    >>> data

       a  b  c
    0  1  2  3
    1  4  5  6
    2  7  8  9


    # Series and DataFrame can export to JSON also
    >>> print(data.to_json())
    {"a":{"0":1,"1":4,"2":7},"b":{"0":2,"1":5,"2":8},"c":{"0":3,"1":6,"2":9}}

    >>> print(data.to_json(orient='records'))
    [{"a":1,"b":2,"c":3},{"a":4,"b":5,"c":6},{"a":7,"b":8,"c":9}]



- XML and HTML Scraping

    Python has many libraries for reading and writing data in XML and HTML.  Popular
      ones include 'lxml' (fast) and 'Beautiful Soup' and 'html5lib' (better at
      handling malformed data).  

    pandas does have a 'read_html' function, which uses either lxml or Beautiful Soup
      under the hood.


    # By default, 'read_html' searches for and tries to parse anything inside of 
    #   <table> tags
    >>> tables = pd.read_html('examples/fdic_failed_bank_list.html')
    >>> len(tables)
    1

    >>> failures = tables[0]
    >>> failures.head()

                          Bank Name             City  ST   CERT  \
    0                   Allied Bank         Mulberry  AR     91   
    1  The Woodbury Banking Company         Woodbury  GA  11297   
    2        First CornerStone Bank  King of Prussia  PA  35312   
    3            Trust Company Bank          Memphis  TN   9956   
    4    North Milwaukee State Bank        Milwaukee  WI  20364   
                     Acquiring Institution        Closing Date       Updated Date  
    0                         Today's Bank  September 23, 2016  November 17, 2016  
    1                          United Bank     August 19, 2016  November 17, 2016  
    2  First-Citizens Bank & Trust Company         May 6, 2016  September 6, 2016  
    3           The Bank of Fayette County      April 29, 2016  September 6, 2016  
    4  First-Citizens Bank & Trust Company      March 11, 2016      June 16, 2016


    # Now, we'll compute the number of bank failures by year
    >>> close_timestamps = pd.to_datetime(failures['Closing Date'])
    >>> close_timestamps.dt.year.value_counts()

    2010    157
    2009    140
    2011     92
    2012     51
    2008     25
            ... 
    2004      4
    2001      4
    2007      3
    2003      3
    2000      2
    Name: Closing Date, Length: 15, dtype: int64



- Parsing XML With LXML.Objectify

    # Here we use 'lxml.objectify' to parse an xml file and get a reference to the
    #   root node
    >>> from lxml import objectify
    >>> path = 'datasets/mta_perf/Performance_MNR.xml'
    >>> parsed = objectify.parse(open(path))
    >>> root = parsed.getroot()

    # Now, we can populate a dict of tag names to data values
    >>> data = []
    >>> skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ', 'DESIRED_CHANGE', 'DECIMAL_PLACES']

    # root.INDICATOR is a generator that returns each <INCICATOR> xml element
    >>> for elt in root.INDICATOR
            el_data = {}
            for child in elt.getchildren():
                if child in skip_fields: continue
                el_data[child.tag] = child.pyval
            data.append(ed_data)

    # Then, we can convert the list of dicts into a DataFrame
    >>> perf = pd.DataFrame(data)


    # 'lxml.objectify' can also parse XML Metadata using the StringIO module
    >>> from io import StringIO
    >>> tag = '<a href="http://www.google.com">Google</a>'
    >>> root = objectify.parse(StringIO(tag)).getroot()

    >>> root.get('href')
    'http://www.google.com'

    >>> root.text
    'Google'



- Binary Data Formats

    pandas objects have a 'to_pickle' method that can be used to serialize objects to disk.
      Note that pickle is recommended only as a short-term storage format.  This is because
      it's not really possible to guarantee that pickle will be stable over time.

    # Pickle an object
    >>> frame = pd.read_csv('examples/ex1.csv')
    >>> frame.to_pickle('examples/frame_pickle')

    # Unpickle an object
    >>> frame = pd.read_pickle('examples/frame_pickle')


    Other binary formats that can be used to store pandas objects:

      1. bcolz   = compressable column-oriented binary format based on the Blosc compression
                     library

      2. Feather = cross-language column-oriented file format, uses Apache Arrow columnar
                     memory format



- HDF5

    HDF5 ('Hierarchical Data Format') is a well-regarded file format intended for storing 
      large quantities of scientific array data.  It is available as a C library, and has 
      interfaces for many other languages.  

    Each HDF5 file can store multiple datasets and supporting metadata.  It supports
      on-the-fly compression with a variety of compression modes.  It is a good choice for
      working with very large datasets that don't fit into memory, since you can read
      and write small sections of much larger arrays.  Since many data science problems
      are IO-bound (not CPU-bound), using something like HDF5 can massively accelerate
      your applications.  

    Note that HDF5 is not a database.  It is best suited for write-once, read-many
      datasets.  Concurrent writes can corrupt a file.

    Python has the 'PyTables' and 'h5py' libraries for working with HDF5 files directly,
      and pandas provides a high-level interface that simplifies storing pandas objects.


    # The HDFStore class works like a dict
    >>> frame = pd.DataFrame({'a': np.random.randn(100)})
    >>> store = pd.HDFStore('mydata.h5')

    # Add the entire DataFrame or just a column to the store
    >>> store['obj1'] = frame
    >>> store['obj1_col'] = frame['a']

    # To retrive an object from the store
    >>> frame = store['obj1']


    # The HDFStore supports 2 storage schemas, 'fixed' and 'table'
    #   'table' is slower, but it supports query operations
    >>> store.put('obj2', frame, format='table')
    >>> store.select('obj2', where=['index >= 10 and index <= 15'])

               a
    10  1.007189
    11 -1.296221
    12  0.274992
    13  0.228913
    14  1.352917
    15  0.886429

    >>> store.close()


    # The pandas 'read_hdf' and 'to_hdr' functions give you shortcuts to these tools
    >>> frame.to_hdf('mydata.h5', 'obj3', format='table')
    >>> pd.read_hdf('mydata.h5', 'obj3', where=['index < 5'])



- Reading Excel Files

    pandas supports reading from Excel files (2003+) using the 'ExcelFile' class or
      the built-in 'read_excel' function.  These tools use the 'xlrd' and 'openpyxl'
      libraries under the hood.


    # Read an excel file
    >>> xlsx = pd.ExcelFile('examples/ex1.xslx')
    >>> df = pd.read_excel(xslx, 'Sheet1')

    # Write to an excel file
    >>> writer = pd.ExcelWriter('examples/ex2.xslx')
    >>> frame.to_excel(writer, 'Sheet1')
    >>> writer.save()

    # More concise write syntax
    >>> frame.to_excel('examples/ex2.xslx')


- Interacting With Web APIs

    There are a number of ways to interact with web APIs from Python.  One of the most
      common is the 'requests' package.


    # Get the last 30 issues for pandas on GitHub
    >>> import requests
    >>> url = 'https://api.github.com/repos/pandas-dev/pandas/issues'
    >>> resp = requests.get(url)
    >>> resp
    <Response [200]>

    # Get the json returned in the response
    >>> data = resp.json()
    >>> data[0]['title']
    'BUG: rank with +-inf, #6945'

    # Convert the json dictionary to a DataFrame
    >>> issues = pd.DataFrame(data, columns=['number', 'title', 'labels', 'state'])



- Interacting With Databases

    # Create a sqlite db
    >>> import sqlite3
    >>> query = query = """
    .....: CREATE TABLE test
    .....: (a VARCHAR(20), b VARCHAR(20),
    .....:  c REAL,        d INTEGER
    .....: );"""

    >>> con = sqlite3.connect('mydata.sqlite')
    >>> con.commit()


    # Insert some data
    >>> data = [('Atlanta', 'Georgia', 1.25, 6),
    .....:         ('Tallahassee', 'Florida', 2.6, 3),
    .....:         ('Sacramento', 'California', 1.7, 5)]

    >>> stmt = "INSERT INTO test VALUES(?, ?, ?, ?)"
    >>> con.executemany(stmt, data)
    >>> con.commit()


    # Most Python SQL drivers return a list of tuples when selecting from a table
    >>> cursor = conn.execute('select * from test')
    >>> rows = cursor.fetchall()
    >>> rows

    [('Atlanta', 'Georgia', 1.25, 6),
     ('Tallahassee', 'Florida', 2.6, 3),
     ('Sacramento', 'California', 1.7, 5)]


    # The tuples in 'rows' can be passed to the DataFrame constructor, but you also
    #   need the column names, which are contained in the cursor's 'description'
    #   attribute.
    >>> pd.DataFrame(rows, columns=[x[0] for x in cursor.description])



- Using SQLAlchemy

    The 'SQLAlchemy' toolkit abstracts away many of the common differences between
      SQL databases, and using it with pandas can greatly simplify you database
      code.

    >>> import sqlalchemy as sqla
    >>> db = sqla.create_engine('sqlite:///mydata.sqlite')

    >>> pd.read_sql('select * from test', db)