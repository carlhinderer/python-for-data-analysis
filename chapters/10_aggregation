------------------------------------------------------
CHAPTER 10 - DATA AGGREGATION AND GROUP OPERATIONS
------------------------------------------------------

- GroupBy Mechanics

    Hadley Wickham, an author of many popular packages for the R programming language, 
      coined the term 'split-apply-combine' for describing group operations.

      1. Data contained in a pandas object, whether a Series, DataFrame, or otherwise, 
          is split into groups based on one or more keys that you provide. The splitting 
          is performed on a particular axis of an object. For example, a DataFrame can be 
          grouped on its rows (axis=0) or its columns (axis=1). 

      2. Once this is done, a function is applied to each group, producing a new value. 

      3. The results of all those function applications are combined into a result object. 
           The form of the resulting object will usually depend on whatâ€™s being done to the data.


      key  | A   B   C   A   B   C   A   B   C
      data | 0   5  10   5  10  15  10  15  20 

                          |
                          |    SPLIT
                          |
                         \ /

         A   A   A       B   B   B       C   C   C
         0   5  10       5  10  15      10  15  20

                          |
                          |    APPLY
                          |
                         \ /

                         sum()

                          |
                          |    COMBINE
                          |
                         \ /

                      A   B   C
                     15  30  45



- Grouping Keys

    Each grouping key can take many forms, and the keys don't all have to be the same
      type.  Different types of keys include:

      1. A list or array of values the same length as the axis being grouped
      2. A value indicating a column name in a DataFrame
      3. A dict or Series giving a correspondence between the values on the axis being grouped
      4. A function to be invoked on the axis index or the individual labels in the index


    # Create a DataFrame
    >>> df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
                           'key2' : ['one', 'two', 'one', 'two', 'one'],
                           'data1' : np.random.randn(5),
                           'data2' : np.random.randn(5)})
    >>> df

          data1     data2 key1 key2
    0 -0.204708  1.393406    a  one
    1  0.478943  0.092908    a  two
    2 -0.519439  0.281746    b  one
    3 -0.555730  0.769023    b  two
    4  1.965781  1.246435    a  one


    # Suppose we want to compute the mean of the 'data1' column using the labels from 'key1'
    >>> grouped = df['data1'].groupby(df['key1'])
    >>> grouped
    <pandas.core.groupby.SeriesGroupBy object at 0x7f85008d0400>

    >>> grouped.mean()

    key1
    a    0.746672
    b   -0.537585
    Name: data1, dtype: float64


    # We can also pass multiple arrays as keys
    >>> means = df['data1'].groupby([df['key1'], df['key2']]).mean()
    >>> means

    key1  key2
    a     one     0.880536
          two     0.478943
    b     one    -0.519439
          two    -0.555730
    Name: data1, dtype: float64

    >>> means.unstack()

    key2       one       two
    key1                    
    a     0.880536  0.478943
    b    -0.519439 -0.555730



- Using numpy Arrays as Grouping Keys

    # Create numpy arrays
    >>> states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])
    >>> years = np.array([2005, 2005, 2006, 2005, 2006])
    
    # Use numpy arrays as grouping keys
    >>> df['data1'].groupby([states, years]).mean()

    California  2005    0.478943
                2006   -0.519439
    Ohio        2005   -0.380219
                2006    1.965781
    Name: data1, dtype: float64



- Using DataFrame Columns as Grouping Keys

    Frequently, the grouping information is found in the same DataFrame as the data you want
      to work on.  In that case, you can pass column names as group keys.

    >>> df.groupby('key1').mean()

             data1     data2
    key1                    
    a     0.746672  0.910916
    b    -0.537585  0.525384
    
    >>> df.groupby(['key1', 'key2']).mean()
    Out[22]: 
                  data1     data2
    key1 key2                    
    a    one   0.880536  1.319920
         two   0.478943  0.092908
    b    one  -0.519439  0.281746
         two  -0.555730  0.769023


    In the first case above, 'df.groupby('key1').mean()' did not include 'key2' in the
      grouped data.  This is because 'key2' is not numeric data, so it is said to be a
      'nuissance column', and is therefore excluded from the result.



- Getting Group Sizes

    # The 'size' method is used to get group sizes
    >>> df.groupby(['key1', 'key2']).size()

    key1  key2
    a     one     2
          two     1
    b     one     1
          two     1
    dtype: int64



- Iterating Over Groups

    The 'GroupBy' object is iterable, generating a sequence of 2-tuples containing the 
      group name along with the chunk of data.

    >>> for name, group in df.groupby('key1'):
            print(name)
            print(group)

    a
          data1     data2 key1 key2
    0 -0.204708  1.393406    a  one
    1  0.478943  0.092908    a  two
    4  1.965781  1.246435    a  one

    b
          data1     data2 key1 key2
    2 -0.519439  0.281746    b  one
    3 -0.555730  0.769023    b  two


    In the case of multiple keys, the first element in the tuple will be a tuple of key
      values.

    >>> for (k1, k2), group in df.groupby(['key1', 'key2']):
            print((k1, k2))
            print(group)

    ('a', 'one')
          data1     data2 key1 key2
    0 -0.204708  1.393406    a  one
    4  1.965781  1.246435    a  one

    ('a', 'two')
          data1     data2 key1 key2
    1  0.478943  0.092908    a  two

    ('b', 'one')
          data1     data2 key1 key2
    2 -0.519439  0.281746    b  one

    ('b', 'two')
         data1     data2 key1 key2
    3 -0.55573  0.769023    b  two



- Group Iteration Patterns

    # Compute a dict of data pieces as a one-liner
    >>> pieces = dict(list(df.groupby('key1')))
    >>> pieces['b']

          data1     data2 key1 key2
    2 -0.519439  0.281746    b  one
    3 -0.555730  0.769023    b  two


    # Group on object types on column axis
    >>> df.types

    data1    float64
    data2    float64
    key1      object
    key2      object
    dtype: object

    >>> grouped = df.groupby(df.dtypes, axis=1)
    >>> for dtype, group in grouped:
            print(dtype)
            print(group)

    float64
          data1     data2
    0 -0.204708  1.393406
    1  0.478943  0.092908
    2 -0.519439  0.281746
    3 -0.555730  0.769023
    4  1.965781  1.246435

    object
      key1 key2
    0    a  one
    1    a  two
    2    b  one
    3    b  two
    4    a  one



- Selecting a Column or Subset of Columns

    Indexing a GroupBy object created from a DataFrame with a column name or array of
      column names has the effect of column subsetting for aggregation.

    # These are equivalent
    >>> df['data1'].groupby(df['key1'])
    >>> df.groupby('key1')['data1']

    # These are equivalent
    >>> df[['data2']].groupby(df['key1'])
    >>> df.groupby('key1')[['data2']]


    For large datasets, it may be desirable to aggregate only a few columns.

    # Compute means for just the 'data2' column
    >>> df.groupby(['key1', 'key2'])[['data2']].mean()

                  data2
    key1 key2          
    a    one   1.319920
         two   0.092908
    b    one   0.281746
         two   0.769023



- Grouping With Dicts

    Grouping information may exist in a form other than an array.


    # Create a DataFrame and add a few NA values
    >>> people = pd.DataFrame(np.random.randn(5, 5),
                              columns=['a', 'b', 'c', 'd', 'e'],
                              index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])
    >>> people.iloc[2:3, [1, 2]] = np.nan
    >>> people

                   a         b         c         d         e
    Joe     1.007189 -1.296221  0.274992  0.228913  1.352917
    Steve   0.886429 -2.001637 -0.371843  1.669025 -0.438570
    Wes    -0.539741       NaN       NaN -1.021228 -0.577087
    Jim     0.124121  0.302614  0.523772  0.000940  1.343810
    Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757


    # Now, supposed we have a group correspondence for the columns and want to sum
    #   together the columns by group.
    >>> mapping = {'a': 'red', 'b': 'red', 'c': 'blue', 'd': 'blue', 'e': 'red', 'f' : 'orange'}


    # We can pass this dict directly to 'groupby'
    >>> by_column = people.groupby(mapping, axis=1)
    >>> by_column.sum()

                blue       red
    Joe     0.503905  1.063885
    Steve   1.297183 -1.553778
    Wes    -1.021228 -1.116829
    Jim     0.524712  1.770545
    Travis -4.230992 -2.405455



- Grouping With Series

    # Create a Series
    >>> map_series = pd.Series(mapping)
    >>> map_series

    a       red
    b       red
    c      blue
    d      blue
    e       red
    f    orange
    dtype: object


    # Grouping by Series works also, since everything gets converted to arrays internally    
    >>> people.groupby(map_series, axis=1).count()

            blue  red
    Joe        2    3
    Steve      2    3
    Wes        1    2
    Jim        2    3
    Travis     2    3



- Grouping With Functions

    Using functions is a more generic way of defining a group mapping.  Any function 
      passed in as a group key will be called once per index value, with the return
      values being used as the group names.


    # Group by person's name length
    >>> people.groupby(len).sum()

              a         b         c         d         e
    3  0.591569 -0.993608  0.798764 -0.791374  2.119639
    5  0.886429 -2.001637 -0.371843  1.669025 -0.438570
    6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757
    

    # Can combine with other grouping keys
    >>> key_list = ['one', 'one', 'one', 'two', 'two']
    >>> people.groupby([len, key_list]).min()

                  a         b         c         d         e
    3 one -0.539741 -1.296221  0.274992 -1.021228 -0.577087
      two  0.124121  0.302614  0.523772  0.000940  1.343810
    5 one  0.886429 -2.001637 -0.371843  1.669025 -0.438570
    6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757



- Grouping By Index Levels

    We can also aggregate on one of the levels of a hierarchical axis index.


    # Create a DataFrame with a hierarchical index
    >>> columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],
                                            [1, 3, 5, 1, 3]],
                                            names=['cty', 'tenor'])

    >>> hier_df = pd.DataFrame(np.random.randn(4, 5), columns=columns)
    >>> hier_df
 
    cty          US                            JP          
    tenor         1         3         5         1         3
    0      0.560145 -1.265934  0.119827 -1.063512  0.332883
    1     -2.359419 -0.199543 -1.541996 -0.970736 -1.307030
    2      0.286350  0.377984 -0.753887  0.331286  1.349742
    3      0.069877  0.246674 -0.011862  1.004812  1.327195
    
    
    # To group by level, pass the level number or name using the 'level' keyword
    >>> hier_df.groupby(level='cty', axis=1).count()

    cty  JP  US
    0     2   3
    1     2   3
    2     2   3
    3     2   3



- Data Aggregations

    Aggregations refer to any data transformation that produces scalar values from
      arrays.  Many common aggregations have optimized implementations.


    Name           Description
    ------------------------------------------------------------------------
    count          Number of non-NA values in the group
    sum            Sum of non-NA values
    mean           Mean of non-NA values
    median         Arithmetic median of non-NA values
    std, var       Unbiased (n â€“ 1 denominator) standard deviation and variance
    min, max       Minimum and maximum of non-NA values
    prod           Product of non-NA values
    first, last    First and last non-NA values



- Non-Optimized Aggregations

    We can also use any other method for our aggregations.  For example, we can use the
      'Series.quantile' method.  Internally, GroupBy efficiently slices up the Series,
      calls 'piece.quantile(0.9)', then assembles those results together into the
      result object.


    # Use a previous DataFrame
    >>> df
    
          data1     data2 key1 key2
    0 -0.204708  1.393406    a  one
    1  0.478943  0.092908    a  two
    2 -0.519439  0.281746    b  one
    3 -0.555730  0.769023    b  two
    4  1.965781  1.246435    a  one
    

    >>> grouped = df.groupby('key1')
    >>> grouped['data1'].quantile(0.9)

    key1
    a    1.668413
    b   -0.523068
    Name: data1, dtype: float64



- User-Defined Aggregations

    # We can pass any function that aggregates an array to the 'agg' method
    >>> def peak_to_peak(arr):
            return arr.max() - arr.min()

    >>> grouped.agg(peak_to_peak)

             data1     data2
    key1                    
    a     2.170488  1.300498
    b     0.036292  0.487276



- Column-Wise and Multiple Function Application

    # Load tipping dataset from earlier examples
    >>> tips = pd.read_csv('examples/tips.csv')

    # Add tip percentage of total bill
    >>> tips['tip_pct'] = tips['tip'] / tips['total_bill']

    >>> tips[:6]

       total_bill   tip smoker  day    time  size   tip_pct
    0       16.99  1.01     No  Sun  Dinner     2  0.059447
    1       10.34  1.66     No  Sun  Dinner     3  0.160542
    2       21.01  3.50     No  Sun  Dinner     3  0.166587
    3       23.68  3.31     No  Sun  Dinner     2  0.139780
    4       24.59  3.61     No  Sun  Dinner     4  0.146808
    5       25.29  4.71     No  Sun  Dinner     4  0.186240


    Sometimes, we may want to use different aggregation functions on different columns,
      or multiple aggregation functions at once.


    # Group the tips by day and smoker
    >>> grouped = tips.groupby(['day', 'smoker'])
    >>> grouped_pct = grouped['tip_pct']

    >>> grouped_pct.agg('mean')

    day   smoker
    Fri   No        0.151650
          Yes       0.174783
    Sat   No        0.158048
          Yes       0.147906
    Sun   No        0.160113
          Yes       0.187250
    Thur  No        0.160298
          Yes       0.163863
    Name: tip_pct, dtype: float64


    # Use different functions on different columns
    >>> grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])

                      foo       bar
    day  smoker                    
    Fri  No      0.151650  0.028123
         Yes     0.174783  0.051293
    Sat  No      0.158048  0.039767
         Yes     0.147906  0.061375
    Sun  No      0.160113  0.042347
         Yes     0.187250  0.154134
    Thur No      0.160298  0.038774
         Yes     0.163863  0.039389


    # Use several functions on the same columns, resulting in 
    #   hierarchical column index
    >>> functions = ['count', 'mean', 'max']
    >>> result = grouped['tip_pct', 'total_bill'].agg(functions)
    >>> result

                tip_pct                     total_bill                  
                  count      mean       max      count       mean    max
    day  smoker                                                         
    Fri  No           4  0.151650  0.187735          4  18.420000  22.75
         Yes         15  0.174783  0.263480         15  16.813333  40.17
    Sat  No          45  0.158048  0.291990         45  19.661778  48.33
         Yes         42  0.147906  0.325733         42  21.276667  50.81
    Sun  No          57  0.160113  0.252672         57  20.506667  48.17
         Yes         19  0.187250  0.710345         19  24.120000  45.35
    Thur No          45  0.160298  0.266312         45  17.113111  41.19
         Yes         17  0.163863  0.241255         17  19.190588  43.11


    >>> result['tip_pct']

                 count      mean       max
    day  smoker                           
    Fri  No          4  0.151650  0.187735
         Yes        15  0.174783  0.263480
    Sat  No         45  0.158048  0.291990
         Yes        42  0.147906  0.325733
    Sun  No         57  0.160113  0.252672
         Yes        19  0.187250  0.710345
    Thur No         45  0.160298  0.266312
         Yes        17  0.163863  0.241255



- Advanced Multiple Function Application

    # As before, a list of tuples with custom names can be passed
    >>> ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]
    >>> grouped['tip_pct', 'total_bill'].agg(ftuples)
 
                     tip_pct              total_bill            
                Durchschnitt Abweichung Durchschnitt  Abweichung
    day  smoker                                                 
    Fri  No         0.151650   0.000791    18.420000   25.596333
         Yes        0.174783   0.002631    16.813333   82.562438
    Sat  No         0.158048   0.001581    19.661778   79.908965
         Yes        0.147906   0.003767    21.276667  101.387535
    Sun  No         0.160113   0.001793    20.506667   66.099980
         Yes        0.187250   0.023757    24.120000  109.046044
    Thur No         0.160298   0.001503    17.113111   59.625081
         Yes        0.163863   0.001551    19.190588   69.808518


    # If we want to apply different functions to one or more of the columns, we can
    #   pass a dict to 'agg' that contains a mapping of column names to any of the
    #   function specifications.
    >>> grouped.agg({'tip' : np.max, 'size' : 'sum'})

                   tip  size
    day  smoker             
    Fri  No       3.50     9
         Yes      4.73    31
    Sat  No       9.00   115
         Yes     10.00   104
    Sun  No       6.00   167
         Yes      6.50    49
    Thur No       6.70   112
         Yes      5.00    40


    >>> grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'], 'size' : 'sum'})
    
                  tip_pct                               size
                      min       max      mean       std  sum
    day  smoker                                             
    Fri  No      0.120385  0.187735  0.151650  0.028123    9
         Yes     0.103555  0.263480  0.174783  0.051293   31
    Sat  No      0.056797  0.291990  0.158048  0.039767  115
         Yes     0.035638  0.325733  0.147906  0.061375  104
    Sun  No      0.059447  0.252672  0.160113  0.042347  167
         Yes     0.065660  0.710345  0.187250  0.154134   49
    Thur No      0.072961  0.266312  0.160298  0.038774  112
         Yes     0.090014  0.241255  0.163863  0.039389   40



- Returning Aggregated Data Without Row Indexes

    In all examples up until now, the aggregated data comes back with an index, potentially
      hierarchical, composed from the unique group key combinations.  If we don't want the
      indexes, we can pass the 'as_index=False' argument to 'groupby'.


    >>> tips.groupby(['day', 'smoker'], as_index=False).mean()

        day smoker  total_bill       tip      size   tip_pct
    0   Fri     No   18.420000  2.812500  2.250000  0.151650
    1   Fri    Yes   16.813333  2.714000  2.066667  0.174783
    2   Sat     No   19.661778  3.102889  2.555556  0.158048
    3   Sat    Yes   21.276667  2.875476  2.476190  0.147906
    4   Sun     No   20.506667  3.167895  2.929825  0.160113
    5   Sun    Yes   24.120000  3.516842  2.578947  0.187250
    6  Thur     No   17.113111  2.673778  2.488889  0.160298
    7  Thur    Yes   19.190588  3.030000  2.352941  0.163863



- Apply: General Split-Apply-Combine

    Suppose we're using the tipping dataset, and we want to find the top five 'tip_pct'
      values by group.  First, we write a function that selects the rows with the largest
      values in a particular column.

    >>> def top(df, n=5, column='tip_pct'):
            return df.sort_values(by=column)[-n:]

    >>> top(tips, n=6)

         total_bill   tip smoker  day    time  size   tip_pct
    109       14.31  4.00    Yes  Sat  Dinner     2  0.279525
    183       23.17  6.50    Yes  Sun  Dinner     4  0.280535
    232       11.61  3.39     No  Sat  Dinner     2  0.291990
    67         3.07  1.00    Yes  Sat  Dinner     1  0.325733
    178        9.60  4.00    Yes  Sun  Dinner     2  0.416667
    172        7.25  5.15    Yes  Sun  Dinner     2  0.710345


    Now, if we group by smoker, and call 'apply' with this function, we get:

    >>> tips.groupby('smoker').apply(top)

                total_bill   tip smoker   day    time  size   tip_pct
    smoker                                                           
    No     88        24.71  5.85     No  Thur   Lunch     2  0.236746
           185       20.69  5.00     No   Sun  Dinner     5  0.241663
           51        10.29  2.60     No   Sun  Dinner     2  0.252672
           149        7.51  2.00     No  Thur   Lunch     2  0.266312
           232       11.61  3.39     No   Sat  Dinner     2  0.291990
    Yes    109       14.31  4.00    Yes   Sat  Dinner     2  0.279525
           183       23.17  6.50    Yes   Sun  Dinner     4  0.280535
           67         3.07  1.00    Yes   Sat  Dinner     1  0.325733
           178        9.60  4.00    Yes   Sun  Dinner     2  0.416667
           172        7.25  5.15    Yes   Sun  Dinner     2  0.710345


    Here, the 'top' function is called on each row group from the DataFrame, and the 
      results are glued together using 'pandas.concat', labeling the pieces with the group
      names.  

    If you pass a function to 'apply' that takes other arguments, you can pass these
      after the function:

    >>> tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')
 
                     total_bill    tip smoker   day    time  size   tip_pct
    smoker day                                                             
    No     Fri  94        22.75   3.25     No   Fri  Dinner     2  0.142857
           Sat  212       48.33   9.00     No   Sat  Dinner     4  0.186220
           Sun  156       48.17   5.00     No   Sun  Dinner     6  0.103799
           Thur 142       41.19   5.00     No  Thur   Lunch     5  0.121389
    Yes    Fri  95        40.17   4.73    Yes   Fri  Dinner     4  0.117750
           Sat  170       50.81  10.00    Yes   Sat  Dinner     3  0.196812
           Sun  182       45.35   3.50    Yes   Sun  Dinner     3  0.077178
           Thur 197       43.11   5.00    Yes  Thur   Lunch     4  0.115982



- Calling 'describe' on a 'GroupBy' Object

    Earlier, we called 'describe' on a 'GroupBy' object.

    >>> result = tips.groupby('smoker')['tip_pct'].describe()
    >>> result

            count      mean       std       min       25%       50%       75%  \
    smoker                                                                      
    No      151.0  0.159328  0.039910  0.056797  0.136906  0.155625  0.185014   
    Yes      93.0  0.163196  0.085119  0.035638  0.106771  0.153846  0.195059   
                 max  
    smoker            
    No      0.291990  
    Yes     0.710345  
    

    >>> result.unstack('smoker')

           smoker
    count  No        151.000000
           Yes        93.000000
    mean   No          0.159328
           Yes         0.163196
    std    No          0.039910
           Yes         0.085119
    min    No          0.056797
           Yes         0.035638
    25%    No          0.136906
           Yes         0.106771
    50%    No          0.155625
           Yes         0.153846
    75%    No          0.185014
           Yes         0.195059
    max    No          0.291990
           Yes         0.710345
    dtype: float64


    When we invoke a method like 'describe' on a 'GroupBy' object, it is actually just
      a shortcut for:

    >>> f = lambda x: x.describe()
    >>> grouped.apply(f)



- Suppressing the Group Keys

    When we use 'groupby', the resulting object has a hierarchical index formed from the 
      group keys along with the indexes of each piece of the original object.  To disable
      this, we can pass 'group_keys=False'.


    >>> tips.groupby('smoker', group_keys=False).apply(top)

         total_bill   tip smoker   day    time  size   tip_pct
    88        24.71  5.85     No  Thur   Lunch     2  0.236746
    185       20.69  5.00     No   Sun  Dinner     5  0.241663
    51        10.29  2.60     No   Sun  Dinner     2  0.252672
    149        7.51  2.00     No  Thur   Lunch     2  0.266312
    232       11.61  3.39     No   Sat  Dinner     2  0.291990
    109       14.31  4.00    Yes   Sat  Dinner     2  0.279525
    183       23.17  6.50    Yes   Sun  Dinner     4  0.280535
    67         3.07  1.00    Yes   Sat  Dinner     1  0.325733
    178        9.60  4.00    Yes   Sun  Dinner     2  0.416667
    172        7.25  5.15    Yes   Sun  Dinner     2  0.710345



- Quantile and Bucket Analysis

    We can combine pandas tools like 'cut' and 'qcut' with 'groupby' to conveniently
      perform buck or quantile analysis on a dataset.  


    # Create a random dataset and cut it into equal length buckets
    >>> frame = pd.DataFrame({'data1': np.random.randn(1000),
                              'data2': np.random.randn(1000)})
    >>> quartiles = pd.cut(frame.data1, 4)
    >>> quartiles[:10]

    0     (-1.23, 0.489]
    1    (-2.956, -1.23]
    2     (-1.23, 0.489]
    3     (0.489, 2.208]
    4     (-1.23, 0.489]
    5     (0.489, 2.208]
    6     (-1.23, 0.489]
    7     (-1.23, 0.489]
    8     (0.489, 2.208]
    9     (0.489, 2.208]
    Name: data1, dtype: category
    Categories (4, interval[float64]): [(-2.956, -1.23] < (-1.23, 0.489] < 
    (0.489, 2.208] < (2.208, 3.928]]


    # The 'Categorical' object returned by 'cut' can be passed directly to 'groupby'.
    #   So we can compute a set of statistics for the data2 column like so:
    >>> def get_stats(group):
            return {'min': group.min(), 'max': group.max(), 'count': group.count(), 
                    'mean': group.mean()}

    >>> grouped = frame.data2.groupby(quartiles)
    >>> group.apply(get_stats).unstack()

                     count       max      mean       min
    data1                                               
    (-2.956, -1.23]   95.0  1.670835 -0.039521 -3.399312
    (-1.23, 0.489]   598.0  3.260383 -0.002051 -2.989741
    (0.489, 2.208]   297.0  2.954439  0.081822 -3.745356
    (2.208, 3.928]    10.0  1.765640  0.024750 -1.929776


    # Use 'qcut' to get equal-size buckets instead, and pass 'labels=False' to just
    #   get quantile numbers
    >>> grouping = pd.qcut(frame.data1, 10, labels=False)
    >>> grouped = frame.data2.groupby(grouping)
    >>> grouped.apply(get_stats).unstack()

           count       max      mean       min
    data1                                     
    0      100.0  1.670835 -0.049902 -3.399312
    1      100.0  2.628441  0.030989 -1.950098
    2      100.0  2.527939 -0.067179 -2.925113
    3      100.0  3.260383  0.065713 -2.315555
    4      100.0  2.074345 -0.111653 -2.047939
    5      100.0  2.184810  0.052130 -2.989741
    6      100.0  2.458842 -0.021489 -2.223506
    7      100.0  2.954439 -0.026459 -3.056990
    8      100.0  2.735527  0.103406 -3.745356
    9      100.0  2.377020  0.220122 -2.064111



- Example - Filling Missing Values with Group-Specific Values

- Example - Random Sampling and Permutation

- Example - Group Weighted Average and Correlation

- Example - Group-Wise Linear Regression

- Pivot Tables and Cross-Tabulation

- List of pivot_table Options

- Cross-Tabulations